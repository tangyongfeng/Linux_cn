<!DOCTYPE html>
<html lang="zh">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>如何在 Linux 中找到并删除重复文件</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="Author: Sk 在编辑或修改配置文件或旧文件前，我经常会把它们备份到硬盘的某个地方，因此我如果意外地改错了这些文件，我 …" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">linux_cn</a></h1>
                <nav><ul>
                    <li><a href="/category/chuan-shan-jia-zhuan-fang">穿山甲专访</a></li>
                    <li><a href="/category/dai-ma-ying-xiong">代码英雄</a></li>
                    <li><a href="/category/fen-xiang">分享</a></li>
                    <li><a href="/category/guan-dian">观点</a></li>
                    <li><a href="/category/huo-dong">活动</a></li>
                    <li><a href="/category/ji-ke-man-hua">极客漫画</a></li>
                    <li class="active"><a href="/category/ji-zhu">技术</a></li>
                    <li><a href="/category/kai-yuan-zhi-hui">开源智慧</a></li>
                    <li><a href="/category/linux-fa-xing-ban">Linux 发行版</a></li>
                    <li><a href="/category/mei-ri-an-quan-zi-xun">每日安全资讯</a></li>
                    <li><a href="/category/misc">Misc</a></li>
                    <li><a href="/category/qu-kuai-lian">区块链</a></li>
                    <li><a href="/category/rong-qi-yu-yun">容器与云</a></li>
                    <li><a href="/category/ruan-jian-kai-fa">软件开发</a></li>
                    <li><a href="/category/shu-mei-pai">树莓派</a></li>
                    <li><a href="/category/xi-tong-yun-wei">系统运维</a></li>
                    <li><a href="/category/xin-wen">新闻</a></li>
                    <li><a href="/category/ying-he-guan-cha">硬核观察</a></li>
                    <li><a href="/category/zhi-ye-sheng-ya">职业生涯</a></li>
                    <li><a href="/category/zhong-jiang-ming-dan">中奖名单</a></li>
                    <li><a href="/category/zhuo-mian-ying-yong">桌面应用</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/2018/10/ru-he-zai-linux-zhong-zhao-dao-bing-shan-chu-zhong-fu-wen-jian.html" rel="bookmark"
           title="Permalink to 如何在 Linux 中找到并删除重复文件">如何在 Linux 中找到并删除重复文件</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2018-10-16T17:07:03+02:00">
                Published: Tue 16 October 2018
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/linux-fans-from-china.html">linux fans from china</a>
        </address>
<p>In <a href="/category/ji-zhu">技术</a>.</p>

</footer><!-- /.post-info -->      <p>Author: Sk</p>
<p><img alt="" src="/data/attachment/album/201810/16/170704e7dhu41fqsrqkzf1.png"></p>
<p>在编辑或修改配置文件或旧文件前，我经常会把它们备份到硬盘的某个地方，因此我如果意外地改错了这些文件，我可以从备份中恢复它们。但问题是如果我忘记清理备份文件，一段时间之后，我的磁盘会被这些大量重复文件填满 —— 我觉得要么是懒得清理这些旧文件，要么是担心可能会删掉重要文件。如果你们像我一样，在类 Unix 操作系统中，大量多版本的相同文件放在不同的备份目录，你可以使用下面的工具找到并删除重复文件。</p>
<p><strong>提醒一句：</strong></p>
<p>在删除重复文件的时请尽量小心。如果你不小心，也许会导致<a href="https://www.ostechnix.com/prevent-files-folders-accidental-deletion-modification-linux/">意外丢失数据</a>。我建议你在使用这些工具的时候要特别注意。</p>
<h3>在 Linux 中找到并删除重复文件</h3>
<p>出于本指南的目的，我将讨论下面的三个工具：</p>
<ol>
<li>Rdfind</li>
<li>Fdupes</li>
<li>FSlint</li>
</ol>
<p>这三个工具是自由开源的，且运行在大多数类 Unix 系统中。</p>
<h4>1. Rdfind</h4>
<p><strong>Rdfind</strong> 意即 <strong>r</strong>edundant <strong>d</strong>ata <strong>find</strong>（冗余数据查找），是一个通过访问目录和子目录来找出重复文件的自由开源的工具。它是基于文件内容而不是文件名来比较。Rdfind 使用<strong>排序</strong>算法来区分原始文件和重复文件。如果你有两个或者更多的相同文件，Rdfind 会很智能的找到原始文件并认定剩下的文件为重复文件。一旦找到副本文件，它会向你报告。你可以决定是删除还是使用<a href="https://www.ostechnix.com/explaining-soft-link-and-hard-link-in-linux-with-examples/">硬链接或者符号（软）链接</a>代替它们。</p>
<p><strong>安装 Rdfind</strong></p>
<p>Rdfind 存在于 <a href="https://aur.archlinux.org/packages/rdfind/">AUR</a> 中。因此，在基于 Arch 的系统中，你可以像下面一样使用任一如 <a href="https://www.ostechnix.com/yay-found-yet-another-reliable-aur-helper/">Yay</a> AUR 程序助手安装它。</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>yay<span class="w"> </span>-S<span class="w"> </span>rdfind
</code></pre></div>

<p>在 Debian、Ubuntu、Linux Mint 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>rdfind
</code></pre></div>

<p>在 Fedora 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>rdfind
</code></pre></div>

<p>在 RHEL、CentOS 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>epel-release
$<span class="w"> </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>rdfind
</code></pre></div>

<p><strong>用法</strong></p>
<p>一旦安装完成，仅带上目录路径运行 Rdfind 命令就可以扫描重复文件。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p><img alt="" src="/data/attachment/album/201810/16/170705e3dr7l37atsabsci.png"></p>
<p>正如你看到上面的截屏，Rdfind 命令将扫描 <code>~/Downloads</code> 目录，并将结果存储到当前工作目录下一个名为 <code>results.txt</code> 的文件中。你可以在 <code>results.txt</code> 文件中看到可能是重复文件的名字。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">cat</span><span class="w"> </span><span class="n">results</span><span class="o">.</span><span class="n">txt</span>
<span class="c1"># Automatically generated</span>
<span class="c1"># duptype id depth size device inode priority name</span>
<span class="n">DUPTYPE_FIRST_OCCURRENCE</span><span class="w"> </span><span class="mi">1469</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="mi">2050</span><span class="w"> </span><span class="mi">15864884</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">tor</span><span class="o">-</span><span class="n">browser_en</span><span class="o">-</span><span class="n">US</span><span class="o">/</span><span class="n">Browser</span><span class="o">/</span><span class="n">TorBrowser</span><span class="o">/</span><span class="n">Tor</span><span class="o">/</span><span class="n">PluggableTransports</span><span class="o">/</span><span class="n">fte</span><span class="o">/</span><span class="n">tests</span><span class="o">/</span><span class="n">dfas</span><span class="o">/</span><span class="n">test5</span><span class="o">.</span><span class="n">regex</span>
<span class="n">DUPTYPE_WITHIN_SAME_TREE</span><span class="w"> </span><span class="o">-</span><span class="mi">1469</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="mi">2050</span><span class="w"> </span><span class="mi">15864886</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">tor</span><span class="o">-</span><span class="n">browser_en</span><span class="o">-</span><span class="n">US</span><span class="o">/</span><span class="n">Browser</span><span class="o">/</span><span class="n">TorBrowser</span><span class="o">/</span><span class="n">Tor</span><span class="o">/</span><span class="n">PluggableTransports</span><span class="o">/</span><span class="n">fte</span><span class="o">/</span><span class="n">tests</span><span class="o">/</span><span class="n">dfas</span><span class="o">/</span><span class="n">test6</span><span class="o">.</span><span class="n">regex</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
<span class="n">DUPTYPE_FIRST_OCCURRENCE</span><span class="w"> </span><span class="mi">13</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="mi">403635</span><span class="w"> </span><span class="mi">2050</span><span class="w"> </span><span class="mi">15740257</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>
<span class="n">DUPTYPE_WITHIN_SAME_TREE</span><span class="w"> </span><span class="o">-</span><span class="mi">13</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="mi">403635</span><span class="w"> </span><span class="mi">2050</span><span class="w"> </span><span class="mi">15741071</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="o">.</span><span class="n">pdf</span>
<span class="c1"># end of file</span>
</code></pre></div>

<p>通过检查 <code>results.txt</code> 文件，你可以很容易的找到那些重复文件。如果愿意你可以手动的删除它们。</p>
<p>此外，你可在不修改其他事情情况下使用 <code>-dryrun</code> 选项找出所有重复文件，并在终端上输出汇总信息。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">-</span><span class="n">dryrun</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>一旦找到重复文件，你可以使用硬链接或符号链接代替他们。</p>
<p>使用硬链接代替所有重复文件，运行：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">-</span><span class="n">makehardlinks</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>使用符号链接/软链接代替所有重复文件，运行：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">-</span><span class="n">makesymlinks</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>目录中有一些空文件，也许你想忽略他们，你可以像下面一样使用 <code>-ignoreempty</code> 选项：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">-</span><span class="n">ignoreempty</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>如果你不再想要这些旧文件，删除重复文件，而不是使用硬链接或软链接代替它们。</p>
<p>删除重复文件，就运行：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">-</span><span class="n">deleteduplicates</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>如果你不想忽略空文件，并且和所哟重复文件一起删除。运行：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">rdfind</span><span class="w"> </span><span class="o">-</span><span class="n">deleteduplicates</span><span class="w"> </span><span class="bp">true</span><span class="w"> </span><span class="o">-</span><span class="n">ignoreempty</span><span class="w"> </span><span class="bp">false</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>更多细节，参照帮助部分：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>rdfind<span class="w"> </span>--help
</code></pre></div>

<p>手册页：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>man<span class="w"> </span>rdfind
</code></pre></div>

<h4>2. Fdupes</h4>
<p><strong>Fdupes</strong> 是另一个在指定目录以及子目录中识别和移除重复文件的命令行工具。这是一个使用 C 语言编写的自由开源工具。Fdupes 通过对比文件大小、部分 MD5 签名、全部 MD5 签名，最后执行逐个字节对比校验来识别重复文件。</p>
<p>与 Rdfind 工具类似，Fdupes 附带非常少的选项来执行操作，如：</p>
<ul>
<li>在目录和子目录中递归的搜索重复文件</li>
<li>从计算中排除空文件和隐藏文件</li>
<li>显示重复文件大小</li>
<li>出现重复文件时立即删除</li>
<li>使用不同的拥有者/组或权限位来排除重复文件</li>
<li>更多</li>
</ul>
<p><strong>安装 Fdupes</strong></p>
<p>Fdupes 存在于大多数 Linux 发行版的默认仓库中。</p>
<p>在 Arch Linux 和它的变种如 Antergos、Manjaro Linux 上，如下使用 Pacman 安装它。</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>pacman<span class="w"> </span>-S<span class="w"> </span>fdupes
</code></pre></div>

<p>在 Debian、Ubuntu、Linux Mint 上:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>fdupes
</code></pre></div>

<p>在 Fedora 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>fdupes
</code></pre></div>

<p>在 RHEL、CentOS 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>epel-release
$<span class="w"> </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>fdupes
</code></pre></div>

<p><strong>用法</strong></p>
<p>Fdupes 用法非常简单。仅运行下面的命令就可以在目录中找到重复文件，如：<code>~/Downloads</code>。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>我系统中的样例输出：</p>
<div class="highlight"><pre><span></span><code><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="o">.</span><span class="n">pdf</span>
<span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>
</code></pre></div>

<p>你可以看到，在 <code>/home/sk/Downloads/</code> 目录下有一个重复文件。它仅显示了父级目录中的重复文件。如何显示子目录中的重复文件？像下面一样，使用 <code>-r</code> 选项。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">r</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>现在你将看到 <code>/home/sk/Downloads/</code> 目录以及子目录中的重复文件。</p>
<p>Fdupes 也可用来从多个目录中迅速查找重复文件。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span><span class="w"> </span><span class="o">~/</span><span class="n">Documents</span><span class="o">/</span><span class="n">ostechnix</span>
</code></pre></div>

<p>你甚至可以搜索多个目录，递归搜索其中一个目录，如下：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span><span class="w"> </span><span class="o">-</span><span class="n">r</span><span class="w"> </span><span class="o">~/</span><span class="n">Documents</span><span class="o">/</span><span class="n">ostechnix</span>
</code></pre></div>

<p>上面的命令将搜索 <code>~/Downloads</code> 目录，<code>~/Documents/ostechnix</code> 目录和它的子目录中的重复文件。</p>
<p>有时，你可能想要知道一个目录中重复文件的大小。你可以使用 <code>-S</code> 选项，如下：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">S</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
<span class="mi">403635</span><span class="w"> </span><span class="n">bytes</span><span class="w"> </span><span class="n">each</span><span class="p">:</span>
<span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="o">.</span><span class="n">pdf</span>
<span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>
</code></pre></div>

<p>类似的，为了显示父目录和子目录中重复文件的大小，使用 <code>-Sr</code> 选项。</p>
<p>我们可以在计算时分别使用 <code>-n</code> 和 <code>-A</code> 选项排除空白文件以及排除隐藏文件。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">n</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
<span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">A</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>在搜索指定目录的重复文件时，第一个命令将排除零长度文件，后面的命令将排除隐藏文件。</p>
<p>汇总重复文件信息，使用 <code>-m</code> 选项。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
<span class="mi">1</span><span class="w"> </span><span class="n">duplicate</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="p">(</span><span class="ow">in</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">sets</span><span class="p">),</span><span class="w"> </span><span class="n">occupying</span><span class="w"> </span><span class="mf">403.6</span><span class="w"> </span><span class="n">kilobytes</span>
</code></pre></div>

<p>删除所有重复文件，使用 <code>-d</code> 选项。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>样例输出：</p>
<div class="highlight"><pre><span></span><code><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="w"> </span><span class="n">Fabric</span><span class="w"> </span><span class="n">Installation</span><span class="o">.</span><span class="n">pdf</span>
<span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">sk</span><span class="o">/</span><span class="n">Downloads</span><span class="o">/</span><span class="n">Hyperledger</span><span class="w"> </span><span class="n">Fabric</span><span class="w"> </span><span class="n">Installation</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span>

<span class="n">Set</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">preserve</span><span class="w"> </span><span class="n">files</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">all</span><span class="p">]:</span>
</code></pre></div>

<p>这个命令将提示你保留还是删除所有其他重复文件。输入任一号码保留相应的文件，并删除剩下的文件。当使用这个选项的时候需要更加注意。如果不小心，你可能会删除原文件。</p>
<p>如果你想要每次保留每个重复文件集合的第一个文件，且无提示的删除其他文件，使用 <code>-dN</code> 选项（不推荐）。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">dN</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>当遇到重复文件时删除它们，使用 <code>-I</code> 标志。</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="n">fdupes</span><span class="w"> </span><span class="o">-</span><span class="n">I</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span>
</code></pre></div>

<p>关于 Fdupes 的更多细节，查看帮助部分和 man 页面。</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>fdupes<span class="w"> </span>--help
$<span class="w"> </span>man<span class="w"> </span>fdupes
</code></pre></div>

<h4>3. FSlint</h4>
<p><strong>FSlint</strong> 是另外一个查找重复文件的工具，有时我用它去掉 Linux 系统中不需要的重复文件并释放磁盘空间。不像另外两个工具，FSlint 有 GUI 和 CLI 两种模式。因此对于新手来说它更友好。FSlint 不仅仅找出重复文件，也找出坏符号链接、坏名字文件、临时文件、坏的用户 ID、空目录和非精简的二进制文件等等。</p>
<p><strong>安装 FSlint</strong></p>
<p>FSlint 存在于 <a href="https://aur.archlinux.org/packages/fslint/">AUR</a>，因此你可以使用任一 AUR 助手安装它。</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>yay<span class="w"> </span>-S<span class="w"> </span>fslint
</code></pre></div>

<p>在 Debian、Ubuntu、Linux Mint 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>fslint
</code></pre></div>

<p>在 Fedora 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>dnf<span class="w"> </span>install<span class="w"> </span>fslint
</code></pre></div>

<p>在 RHEL，CentOS 上：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>epel-release
$<span class="w"> </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>fslint
</code></pre></div>

<p>一旦安装完成，从菜单或者应用程序启动器启动它。</p>
<p>FSlint GUI 展示如下：</p>
<p><img alt="" src="/data/attachment/album/201810/16/170706vy0opgozoi3ycypg.png"></p>
<p>如你所见，FSlint 界面友好、一目了然。在 “Search path” 栏，添加你要扫描的目录路径，点击左下角 “Find” 按钮查找重复文件。验证递归选项可以在目录和子目录中递归的搜索重复文件。FSlint 将快速的扫描给定的目录并列出重复文件。</p>
<p><img alt="" src="/data/attachment/album/201810/16/170707dfqw80zq8o8e819h.png"></p>
<p>从列表中选择那些要清理的重复文件，也可以选择 “Save”、“Delete”、“Merge” 和 “Symlink” 操作他们。</p>
<p>在 “Advanced search parameters” 栏，你可以在搜索重复文件的时候指定排除的路径。</p>
<p><img alt="" src="/data/attachment/album/201810/16/170709p5105l91u185dlce.png"></p>
<p><strong>FSlint 命令行选项</strong></p>
<p>FSlint 提供下面的 CLI 工具集在你的文件系统中查找重复文件。</p>
<ul>
<li><code>findup</code> — 查找重复文件</li>
<li><code>findnl</code> — 查找名称规范（有问题的文件名）</li>
<li><code>findu8</code> — 查找非法的 utf8 编码的文件名</li>
<li><code>findbl</code> — 查找坏链接（有问题的符号链接）</li>
<li><code>findsn</code> — 查找同名文件（可能有冲突的文件名）</li>
<li><code>finded</code> — 查找空目录</li>
<li><code>findid</code> — 查找死用户的文件</li>
<li><code>findns</code> — 查找非精简的可执行文件</li>
<li><code>findrs</code> — 查找文件名中多余的空白</li>
<li><code>findtf</code> — 查找临时文件</li>
<li><code>findul</code> — 查找可能未使用的库</li>
<li><code>zipdir</code> — 回收 ext2 目录项下浪费的空间</li>
</ul>
<p>所有这些工具位于 <code>/usr/share/fslint/fslint/fslint</code> 下面。</p>
<p>例如，在给定的目录中查找重复文件，运行：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">share</span><span class="o">/</span><span class="n">fslint</span><span class="o">/</span><span class="n">fslint</span><span class="o">/</span><span class="n">findup</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span><span class="o">/</span>
</code></pre></div>

<p>类似的，找出空目录命令是：</p>
<div class="highlight"><pre><span></span><code><span class="o">$</span><span class="w"> </span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">share</span><span class="o">/</span><span class="n">fslint</span><span class="o">/</span><span class="n">fslint</span><span class="o">/</span><span class="n">finded</span><span class="w"> </span><span class="o">~/</span><span class="n">Downloads</span><span class="o">/</span>
</code></pre></div>

<p>获取每个工具更多细节，例如：<code>findup</code>，运行：</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>/usr/share/fslint/fslint/findup<span class="w"> </span>--help
</code></pre></div>

<p>关于 FSlint 的更多细节，参照帮助部分和 man 页。</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>/usr/share/fslint/fslint/fslint<span class="w"> </span>--help
$<span class="w"> </span>man<span class="w"> </span>fslint
</code></pre></div>

<h5>总结</h5>
<p>现在你知道在 Linux 中，使用三个工具来查找和删除不需要的重复文件。这三个工具中，我经常使用 Rdfind。这并不意味着其他的两个工具效率低下，因为到目前为止我更喜欢 Rdfind。好了，到你了。你的最喜欢哪一个工具呢？为什么？在下面的评论区留言让我们知道吧。</p>
<p>就到这里吧。希望这篇文章对你有帮助。更多的好东西就要来了，敬请期待。</p>
<p>谢谢！</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>