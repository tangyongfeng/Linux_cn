<!DOCTYPE html>
<html lang="zh">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>大数据探索：在树莓派上通过 Apache Spark on YARN 搭建 Hadoop 集群</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="Author: Pk 有些时候我们想从 DQYDJ 网站的数据中分析点有用的东西出来，在过去，我们要用 R 语言提取固定宽度的数据，然后通过 …" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">linux_cn</a></h1>
                <nav><ul>
                    <li><a href="/category/chuan-shan-jia-zhuan-fang">穿山甲专访</a></li>
                    <li><a href="/category/dai-ma-ying-xiong">代码英雄</a></li>
                    <li><a href="/category/fen-xiang">分享</a></li>
                    <li><a href="/category/guan-dian">观点</a></li>
                    <li><a href="/category/huo-dong">活动</a></li>
                    <li><a href="/category/ji-ke-man-hua">极客漫画</a></li>
                    <li><a href="/category/ji-zhu">技术</a></li>
                    <li><a href="/category/kai-yuan-zhi-hui">开源智慧</a></li>
                    <li><a href="/category/linux-fa-xing-ban">Linux 发行版</a></li>
                    <li><a href="/category/mei-ri-an-quan-zi-xun">每日安全资讯</a></li>
                    <li><a href="/category/misc">Misc</a></li>
                    <li><a href="/category/qu-kuai-lian">区块链</a></li>
                    <li><a href="/category/rong-qi-yu-yun">容器与云</a></li>
                    <li><a href="/category/ruan-jian-kai-fa">软件开发</a></li>
                    <li class="active"><a href="/category/shu-mei-pai">树莓派</a></li>
                    <li><a href="/category/xi-tong-yun-wei">系统运维</a></li>
                    <li><a href="/category/xin-wen">新闻</a></li>
                    <li><a href="/category/ying-he-guan-cha">硬核观察</a></li>
                    <li><a href="/category/zhi-ye-sheng-ya">职业生涯</a></li>
                    <li><a href="/category/zhong-jiang-ming-dan">中奖名单</a></li>
                    <li><a href="/category/zhuo-mian-ying-yong">桌面应用</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/2017/05/da-shu-ju-tan-suo-zai-shu-mei-pai-shang-tong-guo-apache-spark-on-yarn-da-jian-hadoop-ji-qun.html" rel="bookmark"
           title="Permalink to 大数据探索：在树莓派上通过 Apache Spark on YARN 搭建 Hadoop 集群">大数据探索：在树莓派上通过 Apache Spark on YARN 搭建 Hadoop 集群</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-05-07T09:27:00+02:00">
                Published: Sun 07 May 2017
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/linux-fans-from-china.html">linux fans from china</a>
        </address>
<p>In <a href="/category/shu-mei-pai">树莓派</a>.</p>

</footer><!-- /.post-info -->      <p>Author: Pk</p>
<p><img alt="" src="/data/attachment/album/201705/07/023744ysdd41g0e4g45d1k.jpg"></p>
<p>有些时候我们想从 DQYDJ 网站的数据中分析点有用的东西出来，在过去，我们要<a href="https://dqydj.com/how-to-import-fixed-width-data-into-a-spreadsheet-via-r-playing-with-ipums-cps-data/">用 R 语言提取固定宽度的数据</a>，然后通过数学建模来分析<a href="http://dqydj.com/negative-income-tax-cost-calculator-united-states/">美国的最低收入补贴</a>，当然也包括其他优秀的方法。</p>
<p>今天我将向你展示对大数据的一点探索，不过有点变化，使用的是全世界最流行的微型电脑————<a href="https://www.raspberrypi.org/">树莓派</a>，如果手头没有，那就看下一篇吧（可能是已经处理好的数据），对于其他用户，请继续阅读吧，今天我们要建立一个树莓派 Hadoop集群！</p>
<h3>I. 为什么要建立一个树莓派的 Hadoop 集群？</h3>
<p><img alt="" src="/data/attachment/album/201705/07/023809pk52e4yff97f2ke4.png"></p>
<p><em>由三个树莓派节点组成的 Hadoop 集群</em></p>
<p>我们对 DQYDJ 的数据做了<a href="https://dqydj.com/finance-calculators-investment-calculators-and-visualizations/">大量的处理工作</a>，但这些还不能称得上是大数据。</p>
<p>和许许多多有争议的话题一样，数据的大小之别被解释成这样一个笑话：</p>
<blockquote>
<p>如果能被内存所存储，那么它就不是大数据。 ————佚名</p>
</blockquote>
<p>似乎这儿有两种解决问题的方法：</p>
<ol>
<li>我们可以找到一个足够大的数据集合，任何家用电脑的物理或虚拟内存都存不下。</li>
<li>我们可以买一些不用特别定制，我们现有数据就能淹没它的电脑：<br>
—— 上手树莓派 2B</li>
</ol>
<p>这个由设计师和工程师制作出来的精致小玩意儿拥有 1GB 的内存， MicroSD 卡充当它的硬盘，此外，每一台的价格都低于 50 美元，这意味着你可以花不到 250 美元的价格搭建一个 Hadoop 集群。</p>
<p>或许天下没有比这更便宜的入场券来带你进入大数据的大门。</p>
<h3>II. 制作一个树莓派集群</h3>
<p>我最喜欢制作的原材料。</p>
<p>这里我将给出我原来为了制作树莓派集群购买原材料的链接，如果以后要在亚马逊购买的话你可先这些链接收藏起来，也是对本站的一点支持。(谢谢)</p>
<ul>
<li><a href="http://amzn.to/2bEFTVh">树莓派 2B 3 块</a></li>
<li><a href="http://amzn.to/2bTo1br">4 层亚克力支架</a></li>
<li><a href="http://amzn.to/2bEGO8g">6 口 USB 转接器</a>，我选了白色 RAVPower 50W 10A 6 口 USB 转接器</li>
<li><a href="http://amzn.to/2cguV9I">MicroSD 卡</a>，这个五件套 32GB 卡非常棒</li>
<li><a href="http://amzn.to/2bX2mwm">短的 MicroUSB 数据线</a>，用于给树莓派供电</li>
<li><a href="http://amzn.to/2bDACQJ">短网线</a></li>
<li>双面胶，我有一些 3M 的，很好用</li>
</ul>
<h4>开始制作</h4>
<ol>
<li>首先，装好三个树莓派，每一个用螺丝钉固定在亚克力面板上。（看下图）</li>
<li>接下来，安装以太网交换机，用双面胶贴在其中一个在亚克力面板上。</li>
<li>用双面胶贴将 USB 转接器贴在一个在亚克力面板使之成为最顶层。</li>
<li>接着就是一层一层都拼好——这里我选择将树莓派放在交换机和USB转接器的底下（可以看看完整安装好的两张截图）</li>
</ol>
<p>想办法把线路放在需要的地方——如果你和我一样购买力 USB 线和网线，我可以将它们卷起来放在亚克力板子的每一层</p>
<p>现在不要急着上电，需要将系统烧录到 SD 卡上才能继续。</p>
<h4>烧录 Raspbian</h4>
<p>按照<a href="https://www.raspberrypi.org/downloads/raspbian/">这个教程</a>将 Raspbian 烧录到三张 SD 卡上，我使用的是 Win7 下的 <a href="https://sourceforge.net/projects/win32diskimager/">Win32DiskImager</a>。</p>
<p>将其中一张烧录好的 SD 卡插在你想作为主节点的树莓派上，连接 USB 线并启动它。</p>
<h4>启动主节点</h4>
<p>这里有<a href="http://www.becausewecangeek.com/building-a-raspberry-pi-hadoop-cluster-part-1/">一篇非常棒的“Because We Can Geek”的教程</a>，讲如何安装 Hadoop 2.7.1，此处就不再熬述。</p>
<p>在启动过程中有一些要注意的地方，我将带着你一起设置直到最后一步，记住我现在使用的 IP 段为 192.168.1.50 – 192.168.1.52，主节点是 .50，从节点是 .51 和 .52，你的网络可能会有所不同，如果你想设置静态 IP 的话可以在评论区看看或讨论。</p>
<p>一旦你完成了这些步骤，接下来要做的就是启用交换文件，Spark on YARN 将分割出一块非常接近内存大小的交换文件，当你内存快用完时便会使用这个交换分区。</p>
<p>（如果你以前没有做过有关交换分区的操作的话，可以看看<a href="https://www.digitalocean.com/community/tutorials/how-to-add-swap-on-ubuntu-14-04">这篇教程</a>，让 <code>swappiness</code> 保持较低水准，因为 MicroSD 卡的性能扛不住）</p>
<p>现在我准备介绍有关我的和“Because We Can Geek”关于启动设置一些微妙的区别。</p>
<p>对于初学者，确保你给你的树莓派起了一个正式的名字——在 <code>/etc/hostname</code> 设置，我的主节点设置为 ‘RaspberryPiHadoopMaster’ ，从节点设置为 ‘RaspberryPiHadoopSlave#’</p>
<p>主节点的 <code>/etc/hosts</code> 配置如下：</p>
<div class="highlight"><pre><span></span><code>#/etc/hosts
127.0.0.1       localhost
::1             localhost ip6-localhost ip6-loopback
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters

192.168.1.50    RaspberryPiHadoopMaster
192.168.1.51    RaspberryPiHadoopSlave1
192.168.1.52    RaspberryPiHadoopSlave2
</code></pre></div>

<p>如果你想让 Hadoop、YARN 和 Spark 运行正常的话，你也需要修改这些配置文件（不妨现在就编辑）。</p>
<p>这是 <code>hdfs-site.xml</code>：</p>
<div class="highlight"><pre><span></span><code><span class="cp">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="cp">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span>
<span class="nt">&lt;configuration&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="w">  </span><span class="nt">&lt;name&gt;</span>fs.default.name<span class="nt">&lt;/name&gt;</span>
<span class="w">  </span><span class="nt">&lt;value&gt;</span>hdfs://RaspberryPiHadoopMaster:54310<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span><span class="w">  </span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="w">  </span><span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
<span class="w">  </span><span class="nt">&lt;value&gt;</span>/hdfs/tmp<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span><span class="w">  </span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>

<p>这是 <code>yarn-site.xml</code> （注意内存方面的改变）：</p>
<div class="highlight"><pre><span></span><code><span class="cp">&lt;?xml version=&quot;1.0&quot;?&gt;</span>
<span class="nt">&lt;configuration&gt;</span>

<span class="cm">&lt;!-- Site specific YARN configuration properties --&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>mapreduce_shuffle<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.nodemanager.resource.memory-mb<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>1024<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>128<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>1024<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>1<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="w">  </span><span class="nt">&lt;property&gt;</span>
<span class="w">    </span><span class="nt">&lt;name&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="nt">&lt;/name&gt;</span>
<span class="w">    </span><span class="nt">&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;</span>
<span class="w">  </span><span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="w">   </span><span class="nt">&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="nt">&lt;/name&gt;</span>
<span class="w">   </span><span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
<span class="w">   </span><span class="nt">&lt;description&gt;</span>Whether<span class="w"> </span>virtual<span class="w"> </span>memory<span class="w"> </span>limits<span class="w"> </span>will<span class="w"> </span>be<span class="w"> </span>enforced<span class="w"> </span>for<span class="w"> </span>containers<span class="nt">&lt;/description&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
<span class="w">   </span><span class="nt">&lt;name&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="nt">&lt;/name&gt;</span>
<span class="w">   </span><span class="nt">&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;</span>
<span class="w">   </span><span class="nt">&lt;description&gt;</span>Ratio<span class="w"> </span>between<span class="w"> </span>virtual<span class="w"> </span>memory<span class="w"> </span>to<span class="w"> </span>physical<span class="w"> </span>memory<span class="w"> </span>when<span class="w"> </span>setting<span class="w"> </span>memory<span class="w"> </span>limits<span class="w"> </span>for<span class="w"> </span>containers<span class="nt">&lt;/description&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="nt">&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span><span class="w">  </span>
<span class="nt">&lt;value&gt;</span>RaspberryPiHadoopMaster:8025<span class="nt">&lt;/value&gt;</span><span class="w">  </span>
<span class="nt">&lt;/property&gt;</span><span class="w">  </span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="nt">&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span class="nt">&lt;/name&gt;</span><span class="w">  </span>
<span class="nt">&lt;value&gt;</span>RaspberryPiHadoopMaster:8030<span class="nt">&lt;/value&gt;</span><span class="w">  </span>
<span class="nt">&lt;/property&gt;</span><span class="w">  </span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="nt">&lt;name&gt;</span>yarn.resourcemanager.address<span class="nt">&lt;/name&gt;</span><span class="w">  </span>
<span class="nt">&lt;value&gt;</span>RaspberryPiHadoopMaster:8040<span class="nt">&lt;/value&gt;</span><span class="w">  </span>
<span class="nt">&lt;/property&gt;</span><span class="w"> </span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>

<p><code>slaves</code>：</p>
<div class="highlight"><pre><span></span><code>RaspberryPiHadoopMaster
RaspberryPiHadoopSlave1
RaspberryPiHadoopSlave2
</code></pre></div>

<p><code>core-site.xml</code>：</p>
<div class="highlight"><pre><span></span><code><span class="cp">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="cp">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span>
<span class="nt">&lt;configuration&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="w">  </span><span class="nt">&lt;name&gt;</span>fs.default.name<span class="nt">&lt;/name&gt;</span>
<span class="w">  </span><span class="nt">&lt;value&gt;</span>hdfs://RaspberryPiHadoopMaster:54310<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span><span class="w">  </span>
<span class="nt">&lt;property&gt;</span><span class="w">  </span>
<span class="w">  </span><span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;</span>
<span class="w">  </span><span class="nt">&lt;value&gt;</span>/hdfs/tmp<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span><span class="w">  </span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>

<h4>设置两个从节点：</h4>
<p>接下来<a href="http://www.becausewecangeek.com/building-a-raspberry-pi-hadoop-cluster-part-2/">按照 “Because We Can Geek”上的教程</a>，你需要对上面的文件作出小小的改动。 在 <code>yarn-site.xml</code> 中主节点没有改变，所以从节点中不必含有这个 <code>slaves</code> 文件。</p>
<h3>III. 在我们的树莓派集群中测试 YARN</h3>
<p>如果所有设备都正常工作，在主节点上你应该执行如下命令：</p>
<div class="highlight"><pre><span></span><code>start-dfs.sh
start-yarn.sh
</code></pre></div>

<p>当设备启动后，以 Hadoop 用户执行，如果你遵循教程，用户应该是 <code>hduser</code>。</p>
<p>接下来执行 <code>hdfs dfsadmin -report</code> 查看三个节点是否都正确启动，确认你看到一行粗体文字 ‘Live datanodes (3)’：</p>
<div class="highlight"><pre><span></span><code>Configured Capacity: 93855559680 (87.41 GB)
Raspberry Pi Hadoop Cluster picture Straight On
Present Capacity: 65321992192 (60.84 GB)
DFS Remaining: 62206627840 (57.93 GB)
DFS Used: 3115364352 (2.90 GB)
DFS Used%: 4.77%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0
————————————————-
Live datanodes (3):
Name: 192.168.1.51:50010 (RaspberryPiHadoopSlave1)
Hostname: RaspberryPiHadoopSlave1
Decommission Status : Normal
</code></pre></div>

<p>你现在可以做一些简单的诸如 ‘Hello, World!’ 的测试，或者直接进行下一步。</p>
<h3>IV. 安装 SPARK ON YARN</h3>
<p>YARN 的意思是另一种非常好用的资源调度器（Yet Another Resource Negotiator），已经作为一个易用的资源管理器集成在 Hadoop 基础安装包中。</p>
<p><a href="https://spark.apache.org/">Apache Spark</a> 是 Hadoop 生态圈中的另一款软件包，它是一个毁誉参半的执行引擎和<a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html">捆绑的 MapReduce</a>。在一般情况下，相对于基于磁盘存储的 MapReduce，Spark 更适合基于内存的存储，某些运行任务能够得到 10-100 倍提升——安装完成集群后你可以试试 Spark 和 MapReduce 有什么不同。</p>
<p>我个人对 Spark 还是留下非常深刻的印象，因为它提供了两种数据工程师和科学家都比较擅长的语言—— Python 和 R。</p>
<p>安装 Apache Spark 非常简单，在你家目录下，<code>wget "为 Hadoop 2.7 构建的 Apache Spark”</code>（<a href="https://spark.apache.org/downloads.html">来自这个页面</a>），然后运行 <code>tar -xzf “tgz 文件”</code>，最后把解压出来的文件移动至 <code>/opt</code>，并清除刚才下载的文件，以上这些就是安装步骤。</p>
<p>我又创建了只有两行的文件 <code>spark-env.sh</code>，其中包含 Spark 的配置文件目录。</p>
<div class="highlight"><pre><span></span><code>SPARK_MASTER_IP=192.168.1.50
SPARK_WORKER_MEMORY=512m
</code></pre></div>

<p>(在 YARN 跑起来之前我不确定这些是否有必要。)</p>
<h3>V. 你好，世界! 为 Apache Spark 寻找有趣的数据集!</h3>
<p>在 Hadoop 世界里面的 ‘Hello, World!’ 就是做单词计数。</p>
<p>我决定让我们的作品做一些内省式……为什么不统计本站最常用的单词呢？也许统计一些关于本站的大数据会更有用。</p>
<p>如果你有一个正在运行的 WordPress 博客，可以通过简单的两步来导出和净化。</p>
<ol>
<li>我使用 <a href="https://wordpress.org/support/plugin/export-to-text">Export to Text</a> 插件导出文章的内容到纯文本文件中</li>
<li>我使用一些<a href="https://pypi.python.org/pypi/bleach">压缩库</a>编写了一个 Python 脚本来剔除 HTML</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">bleach</span>

<span class="c1"># Change this next line to your &#39;import&#39; filename, whatever you would like to strip</span>
<span class="c1"># HTML tags from.</span>
<span class="n">ascii_string</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;dqydj_with_tags.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>


<span class="n">new_string</span> <span class="o">=</span> <span class="n">bleach</span><span class="o">.</span><span class="n">clean</span><span class="p">(</span><span class="n">ascii_string</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">[],</span> <span class="n">attributes</span><span class="o">=</span><span class="p">{},</span> <span class="n">styles</span><span class="o">=</span><span class="p">[],</span> <span class="n">strip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">new_string</span> <span class="o">=</span> <span class="n">new_string</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Change this next line to your &#39;export&#39; filename</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;dqydj_stripped.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">new_string</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>

<p>现在我们有了一个更小的、适合复制到树莓派所搭建的 HDFS 集群上的文件。</p>
<p>如果你不能树莓派主节点上完成上面的操作，找个办法将它传输上去（scp、 rsync 等等），然后用下列命令行复制到 HDFS 上。</p>
<div class="highlight"><pre><span></span><code>hdfs dfs -copyFromLocal dqydj_stripped.txt /dqydj_stripped.txt
</code></pre></div>

<p>现在准备进行最后一步 - 向 Apache Spark 写入一些代码。</p>
<h3>VI. 点亮 Apache Spark</h3>
<p>Cloudera 有个极棒的程序可以作为我们的超级单词计数程序的基础，<a href="https://www.cloudera.com/documentation/enterprise/5-6-x/topics/spark_develop_run.html">你可以在这里找到</a>。我们接下来为我们的内省式单词计数程序修改它。</p>
<p>在主节点上<a href="https://pypi.python.org/pypi/stop-words">安装‘stop-words’</a>这个 python 第三方包，虽然有趣（我在 DQYDJ 上使用了 23,295 次 the 这个单词），你可能不想看到这些语法单词占据着单词计数的前列，另外，在下列代码用你自己的数据集替换所有有关指向 dqydj 文件的地方。</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">from</span> <span class="nn">stop_words</span> <span class="kn">import</span> <span class="n">get_stop_words</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

  <span class="c1"># create Spark context with Spark configuration</span>
  <span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s2">&quot;Spark Count&quot;</span><span class="p">)</span>
  <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

  <span class="c1"># get threshold</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
  <span class="k">except</span><span class="p">:</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="mi">5</span>

  <span class="c1"># read in text file and split each document into words</span>
  <span class="n">tokenized</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span>

  <span class="c1"># count the occurrence of each word</span>
  <span class="n">wordCounts</span> <span class="o">=</span> <span class="n">tokenized</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v1</span><span class="p">,</span><span class="n">v2</span><span class="p">:</span><span class="n">v1</span> <span class="o">+</span><span class="n">v2</span><span class="p">)</span>

  <span class="c1"># filter out words with fewer than threshold occurrences</span>
  <span class="n">filtered</span> <span class="o">=</span> <span class="n">wordCounts</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">pair</span><span class="p">:</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span>

  <span class="nb">print</span> <span class="s2">&quot;*&quot;</span> <span class="o">*</span> <span class="mi">80</span>
  <span class="nb">print</span> <span class="s2">&quot;Printing top words used&quot;</span>
  <span class="nb">print</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span>
  <span class="n">filtered_sorted</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">filtered</span><span class="o">.</span><span class="n">collect</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span> <span class="ow">in</span> <span class="n">filtered_sorted</span><span class="p">:</span> <span class="nb">print</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">count</span><span class="p">)</span>


  <span class="c1"># Remove stop words</span>
  <span class="nb">print</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
  <span class="nb">print</span> <span class="s2">&quot;*&quot;</span> <span class="o">*</span> <span class="mi">80</span>
  <span class="nb">print</span> <span class="s2">&quot;Printing top non-stop words used&quot;</span>
  <span class="nb">print</span> <span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span>
  <span class="c1"># Change this to your language code (see the stop-words documentation)</span>
  <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">get_stop_words</span><span class="p">(</span><span class="s1">&#39;en&#39;</span><span class="p">))</span>
  <span class="n">no_stop_words</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">,</span> <span class="n">filtered_sorted</span><span class="p">)</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span> <span class="ow">in</span> <span class="n">no_stop_words</span><span class="p">:</span> <span class="nb">print</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">count</span><span class="p">)</span>
</code></pre></div>

<p>保存好 wordCount.py，确保上面的路径都是正确无误的。</p>
<p>现在，准备念出咒语，让运行在 YARN 上的 Spark 跑起来，你可以看到我在 DQYDJ 使用最多的单词是哪一个。</p>
<div class="highlight"><pre><span></span><code>/opt/spark-2.0.0-bin-hadoop2.7/bin/spark-submit –master yarn –executor-memory 512m –name wordcount –executor-cores 8 wordCount.py /dqydj_stripped.txt
</code></pre></div>

<h3>VII. 我在 DQYDJ 使用最多的单词</h3>
<p>可能入列的单词有哪一些呢？“can, will, it’s, one, even, like, people, money, don’t, also“.</p>
<p>嘿，不错，“money”悄悄挤进了前十。在一个致力于金融、投资和经济的网站上谈论这似乎是件好事，对吧？</p>
<p>下面是的前 50 个最常用的词汇，请用它们刻画出有关我的文章的水平的结论。</p>
<p><img alt="" src="/data/attachment/album/201705/07/023810wrwuwarhr6z6hpph.png"></p>
<p>我希望你能喜欢这篇关于 Hadoop、YARN 和 Apache Spark 的教程，现在你可以在 Spark 运行和编写其他的应用了。</p>
<p>你的下一步是任务是开始<a href="https://spark.apache.org/docs/2.0.0/api/python/index.html">阅读 pyspark 文档</a>（以及用于其他语言的该库），去学习一些可用的功能。根据你的兴趣和你实际存储的数据，你将会深入学习到更多——有流数据、SQL，甚至机器学习的软件包！</p>
<p>你怎么看？你要建立一个树莓派 Hadoop 集群吗？想要在其中挖掘一些什么吗？你在上面看到最令你惊奇的单词是什么？为什么 'S&amp;P' 也能上榜？</p>
<p>（题图：Pixabay，CC0）</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>