<!DOCTYPE html>
<html lang="zh">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>如何分析博客中最流行的编程语言</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="Author: Serge Mosin 摘要：这篇文章我们将对一些各种各样的博客的流行度相对于他们在谷歌上的排名进行一个分析。所有代码可以 …" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">linux_cn</a></h1>
                <nav><ul>
                    <li><a href="/category/chuan-shan-jia-zhuan-fang">穿山甲专访</a></li>
                    <li><a href="/category/dai-ma-ying-xiong">代码英雄</a></li>
                    <li><a href="/category/fen-xiang">分享</a></li>
                    <li><a href="/category/guan-dian">观点</a></li>
                    <li><a href="/category/huo-dong">活动</a></li>
                    <li><a href="/category/ji-ke-man-hua">极客漫画</a></li>
                    <li><a href="/category/ji-zhu">技术</a></li>
                    <li><a href="/category/kai-yuan-zhi-hui">开源智慧</a></li>
                    <li><a href="/category/linux-fa-xing-ban">Linux 发行版</a></li>
                    <li><a href="/category/mei-ri-an-quan-zi-xun">每日安全资讯</a></li>
                    <li><a href="/category/misc">Misc</a></li>
                    <li><a href="/category/qu-kuai-lian">区块链</a></li>
                    <li><a href="/category/rong-qi-yu-yun">容器与云</a></li>
                    <li class="active"><a href="/category/ruan-jian-kai-fa">软件开发</a></li>
                    <li><a href="/category/shu-mei-pai">树莓派</a></li>
                    <li><a href="/category/xi-tong-yun-wei">系统运维</a></li>
                    <li><a href="/category/xin-wen">新闻</a></li>
                    <li><a href="/category/ying-he-guan-cha">硬核观察</a></li>
                    <li><a href="/category/zhi-ye-sheng-ya">职业生涯</a></li>
                    <li><a href="/category/zhong-jiang-ming-dan">中奖名单</a></li>
                    <li><a href="/category/zhuo-mian-ying-yong">桌面应用</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/2017/11/ru-he-fen-xi-bo-ke-zhong-zui-liu-xing-de-bian-cheng-yu-yan.html" rel="bookmark"
           title="Permalink to 如何分析博客中最流行的编程语言">如何分析博客中最流行的编程语言</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-11-09T18:02:00+01:00">
                Published: Thu 09 November 2017
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/linux-fans-from-china.html">linux fans from china</a>
        </address>
<p>In <a href="/category/ruan-jian-kai-fa">软件开发</a>.</p>

</footer><!-- /.post-info -->      <p>Author: Serge Mosin</p>
<blockquote>
<p>摘要：这篇文章我们将对一些各种各样的博客的流行度相对于他们在谷歌上的排名进行一个分析。所有代码可以在 <a href="https://github.com/Databrawl/blog_analysis">github</a> 上找到。</p>
</blockquote>
<p><img alt="" src="/data/attachment/album/201711/09/180208dcnhwgbjbx1c1sc1.jpg"></p>
<h3>想法来源</h3>
<p>我一直在想，各种各样的博客每天到底都有多少页面浏览量，以及在博客阅读受众中最受欢迎的是什么编程语言。我也很感兴趣的是，它们在谷歌的网站排名是否与它们的受欢迎程度直接相关。</p>
<p>为了回答这些问题，我决定做一个 Scrapy 项目，它将收集一些数据，然后对所获得的信息执行特定的数据分析和数据可视化。</p>
<h3>第一部分：Scrapy</h3>
<p>我们将使用 <a href="https://scrapy.org/">Scrapy</a> 为我们的工作，因为它为抓取和对该请求处理后的反馈进行管理提供了干净和健壮的框架。我们还将使用 <a href="https://github.com/scrapinghub/splash">Splash</a> 来解析需要处理的 Javascript 页面。Splash 使用自己的 Web 服务器充当代理，并处理 Javascript 响应，然后再将其重定向到我们的爬虫进程。</p>
<blockquote>
<p>我这里没有描述 Scrapy 的设置，也没有描述 Splash 的集成。你可以在<a href="https://docs.scrapy.org/en/latest/intro/tutorial.html">这里</a>找到 Scrapy 的示例，而<a href="https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/">这里</a>还有 Scrapy+Splash 指南。</p>
</blockquote>
<h4>获得相关的博客</h4>
<p>第一步显然是获取数据。我们需要关于编程博客的谷歌搜索结果。你看，如果我们开始仅仅用谷歌自己来搜索，比如说查询 “Python”，除了博客，我们还会得到很多其它的东西。我们需要的是做一些过滤，只留下特定的博客。幸运的是，有一种叫做 <a href="https://en.wikipedia.org/wiki/Google_Custom_Search">Google 自定义搜索引擎（CSE）</a>的东西，它能做到这一点。还有一个网站 <a href="http://www.blogsearchengine.org/">www.blogsearchengine.org</a>，它正好可以满足我们需要，它会将用户请求委托给 CSE，这样我们就可以查看它的查询并重复利用它们。</p>
<p>所以，我们要做的是到 <a href="http://www.blogsearchengine.org/">www.blogsearchengine.org</a> 网站，搜索 “python”，并在一侧打开 Chrome 开发者工具中的网络标签页。这截图是我们将要看到的：</p>
<p><img alt="" src="/data/attachment/album/201711/09/181024bgawp0jaw3atqkga.png"></p>
<p>突出显示的是 blogsearchengine 向谷歌委派的一个搜索请求，所以我们将复制它，并在我们的 scraper 中使用。</p>
<p>这个博客抓取爬行器类会是如下这样的:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="n">BlogsSpider</span>(<span class="n">scrapy</span>.<span class="n">Spider</span>):
    <span class="nb">name</span> = <span class="s">&#39;blogs&#39;</span>
    <span class="n">allowed_domains</span> = [<span class="s">&#39;cse.google.com&#39;</span>]

    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">queries</span>):
        <span class="n">super</span>(<span class="n">BlogsSpider</span>, <span class="nb">self</span>).<span class="n">__init__</span>()
        <span class="nb">self</span>.<span class="n">queries</span> = <span class="n">queries</span>
</code></pre></div>

<p>与典型的 Scrapy 爬虫不同，我们的方法覆盖了 <code>__init__</code> 方法，它接受额外的参数 <code>queries</code>，它指定了我们想要执行的查询列表。</p>
<p>现在，最重要的部分是构建和执行这个实际的查询。这个过程放在 <code>start_requests</code> 爬虫的方法里面执行，我们愉快地覆盖它：</p>
<div class="highlight"><pre><span></span><code>    def start_requests(self):
        params_dict = {
            &#39;cx&#39;: [&#39;partner-pub-9634067433254658:5laonibews6&#39;],
            &#39;cof&#39;: [&#39;FORID:10&#39;],
            &#39;ie&#39;: [&#39;ISO-8859-1&#39;],
            &#39;q&#39;: [&#39;query&#39;],
            &#39;sa.x&#39;: [&#39;0&#39;],
            &#39;sa.y&#39;: [&#39;0&#39;],
            &#39;sa&#39;: [&#39;Search&#39;],
            &#39;ad&#39;: [&#39;n9&#39;],
            &#39;num&#39;: [&#39;10&#39;],
            &#39;rurl&#39;: [
                &#39;http://www.blogsearchengine.org/search.html?cx=partner-pub&#39;
                &#39;-9634067433254658%3A5laonibews6&amp;cof=FORID%3A10&amp;ie=ISO-8859-1&amp;&#39;
                &#39;q=query&amp;sa.x=0&amp;sa.y=0&amp;sa=Search&#39;
            ],
            &#39;siteurl&#39;: [&#39;http://www.blogsearchengine.org/&#39;]
        }

        params = urllib.parse.urlencode(params_dict, doseq=True)
        url_template = urllib.parse.urlunparse(
            [&#39;https&#39;, self.allowed_domains[0], &#39;/cse&#39;,
             &#39;&#39;, params, &#39;gsc.tab=0&amp;gsc.q=query&amp;gsc.page=page_num&#39;])
        for query in self.queries:
            for page_num in range(1, 11):
                url = url_template.replace(&#39;query&#39;, urllib.parse.quote(query))
                url = url.replace(&#39;page_num&#39;, str(page_num))
                yield SplashRequest(url, self.parse, endpoint=&#39;render.html&#39;,
                                    args={&#39;wait&#39;: 0.5})
</code></pre></div>

<p>在这里你可以看到相当复杂的 <code>params_dict</code> 字典，它控制所有我们之前找到的 Google CSE URL 的参数。然后我们准备好 <code>url_template</code> 里的一切，除了已经填好的查询和页码。我们对每种编程语言请求 10 页，每一页包含 10 个链接，所以是每种语言有 100 个不同的博客用来分析。</p>
<p>在 <code>42-43</code> 行，我使用一个特殊的类 <code>SplashRequest</code> 来代替 Scrapy 自带的 Request 类。它封装了 Splash 库内部的重定向逻辑，所以我们无需为此担心。十分整洁。</p>
<p>最后，这是解析程序：</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">response</span><span class="p">):</span>
<span class="w">        </span><span class="n">urls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.gs-title.gsc-table-cell-thumbnail&#39;</span><span class="p">)</span><span class="w"> </span>\
<span class="w">            </span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;./a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="w">        </span><span class="n">gsc_fragment</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">fragment</span>
<span class="w">        </span><span class="n">fragment_dict</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">parse_qs</span><span class="p">(</span><span class="n">gsc_fragment</span><span class="p">)</span>
<span class="w">        </span><span class="n">page_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb nb-Type">int</span><span class="p">(</span><span class="n">fragment_dict</span><span class="p">[</span><span class="s1">&#39;gsc.page&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="w">        </span><span class="n">query</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fragment_dict</span><span class="p">[</span><span class="s1">&#39;gsc.q&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="w">        </span><span class="n">page_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">urls</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">url</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
<span class="w">            </span><span class="n">parsed_url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="w">            </span><span class="n">rank</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">page_num</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">page_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span>
<span class="w">            </span><span class="nb">yield</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="s1">&#39;rank&#39;</span><span class="p">:</span><span class="w"> </span><span class="n">rank</span><span class="p">,</span>
<span class="w">                </span><span class="s1">&#39;url&#39;</span><span class="p">:</span><span class="w"> </span><span class="n">parsed_url</span><span class="o">.</span><span class="n">netloc</span><span class="p">,</span>
<span class="w">                </span><span class="s1">&#39;query&#39;</span><span class="p">:</span><span class="w"> </span><span class="n">query</span>
<span class="w">            </span><span class="p">}</span>
</code></pre></div>

<p>所有 Scraper 的核心和灵魂就是解析器逻辑。可以有多种方法来理解响应页面的结构并构建 XPath 查询字符串。您可以使用 <a href="https://doc.scrapy.org/en/latest/topics/shell.html">Scrapy shell</a> 尝试并随时调整你的 XPath 查询，而不用运行爬虫。不过我更喜欢可视化的方法。它需要再次用到谷歌 Chrome 开发人员控制台。只需右键单击你想要用在你的爬虫里的元素，然后按下 Inspect。它将打开控制台，并定位到你指定位置的 HTML 源代码。在本例中，我们想要得到实际的搜索结果链接。他们的源代码定位是这样的:</p>
<p><img alt="" src="/data/attachment/album/201711/09/181052l5g5svh3hqosogyy.png"></p>
<p>在查看这个元素的描述后我们看到所找的 <code>&lt;div&gt;</code> 有一个 <code>.gsc-table-cell-thumbnail</code> CSS 类，它是 <code>.gs-title</code> <code>&lt;div&gt;</code> 的子元素，所以我们把它放到响应对象的 <code>css</code> 方法（<code>46</code> 行）。然后，我们只需要得到博客文章的 URL。它很容易通过<code>'./a/@href'</code> XPath 字符串来获得，它能从我们的 <code>&lt;div&gt;</code> 直接子元素的 <code>href</code> 属性找到。（LCTT 译注：此处图文对不上）</p>
<h4>寻找流量数据</h4>
<p>下一个任务是估测每个博客每天得到的页面浏览量。得到这样的数据有<a href="https://www.labnol.org/internet/find-website-traffic-hits/8008/">各种方式</a>，有免费的，也有付费的。在快速搜索之后，我决定基于简单且免费的原因使用网站 <a href="http://www.statshow.com/">www.statshow.com</a> 来做。爬虫将抓取这个网站，我们在前一步获得的博客的 URL 将作为这个网站的输入参数，获得它们的流量信息。爬虫的初始化是这样的:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="n">TrafficSpider</span>(<span class="n">scrapy</span>.<span class="n">Spider</span>):
    <span class="nb">name</span> = <span class="s">&#39;traffic&#39;</span>
    <span class="n">allowed_domains</span> = [<span class="s">&#39;www.statshow.com&#39;</span>]

    <span class="n">def</span> <span class="n">__init__</span>(<span class="nb">self</span>, <span class="n">blogs_data</span>):
        <span class="n">super</span>(<span class="n">TrafficSpider</span>, <span class="nb">self</span>).<span class="n">__init__</span>()
        <span class="nb">self</span>.<span class="n">blogs_data</span> = <span class="n">blogs_data</span>
</code></pre></div>

<p><code>blogs_data</code> 应该是以下格式的词典列表：<code>{"rank": 70, "url": "www.stat.washington.edu"， "query": "Python"}</code>。</p>
<p>请求构建函数如下：</p>
<div class="highlight"><pre><span></span><code>    def start_requests(self):
        url_template = urllib.parse.urlunparse(
            [&#39;http&#39;, self.allowed_domains[0], &#39;/www/{path}&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;])
        for blog in self.blogs_data:
            url = url_template.format(path=blog[&#39;url&#39;])
            request = SplashRequest(url, endpoint=&#39;render.html&#39;,
                                    args={&#39;wait&#39;: 0.5}, meta={&#39;blog&#39;: blog})
            yield request
</code></pre></div>

<p>它相当的简单，我们只是把字符串 <code>/www/web-site-url/</code> 添加到 <code>'www.statshow.com'</code> URL 中。</p>
<p>现在让我们看一下语法解析器是什么样子的：</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="nf">parse</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">response</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">site_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;box_1&quot;]/span/text()&#39;</span><span class="p">).</span><span class="k">extract</span><span class="p">()</span>
<span class="w">        </span><span class="n">views_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">list</span><span class="p">(</span><span class="k">filter</span><span class="p">(</span><span class="n">lambda</span><span class="w"> </span><span class="nl">r</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;$&#39;</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">r</span><span class="p">,</span><span class="w"> </span><span class="n">site_data</span><span class="p">))</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nl">views_data</span><span class="p">:</span>
<span class="w">            </span><span class="n">blog_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response</span><span class="p">.</span><span class="n">meta</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;blog&#39;</span><span class="p">)</span>
<span class="w">            </span><span class="n">traffic_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{</span>
<span class="w">                </span><span class="s1">&#39;daily_page_views&#39;</span><span class="err">:</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="n">views_data</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">.</span><span class="k">translate</span><span class="p">(</span><span class="err">{</span><span class="n">ord</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="k">None</span><span class="err">}</span><span class="p">)),</span>
<span class="w">                </span><span class="s1">&#39;daily_visitors&#39;</span><span class="err">:</span><span class="w"> </span><span class="nc">int</span><span class="p">(</span><span class="n">views_data</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="p">.</span><span class="k">translate</span><span class="p">(</span><span class="err">{</span><span class="n">ord</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span><span class="err">:</span><span class="w"> </span><span class="k">None</span><span class="err">}</span><span class="p">))</span>
<span class="w">            </span><span class="err">}</span>
<span class="w">            </span><span class="n">blog_data</span><span class="p">.</span><span class="k">update</span><span class="p">(</span><span class="n">traffic_data</span><span class="p">)</span>
<span class="w">            </span><span class="n">yield</span><span class="w"> </span><span class="n">blog_data</span>
</code></pre></div>

<p>与博客解析程序类似，我们只是通过 StatShow 示例的返回页面，然后找到包含每日页面浏览量和每日访问者的元素。这两个参数都确定了网站的受欢迎程度，对于我们的分析只需要使用页面浏览量即可 。</p>
<h3>第二部分：分析</h3>
<p>这部分是分析我们搜集到的所有数据。然后，我们用名为 <a href="https://bokeh.pydata.org/en/latest/">Bokeh</a> 的库来可视化准备好的数据集。我在这里没有给出运行器和可视化的代码，但是它可以在 <a href="https://github.com/Databrawl/blog_analysis">GitHub repo</a> 中找到，包括你在这篇文章中看到的和其他一切东西。</p>
<blockquote>
<p>最初的结果集含有少许偏离过大的数据，（如 google.com、linkedin.com、Oracle.com 等）。它们显然不应该被考虑。即使其中有些有博客，它们也不是针对特定语言的。这就是为什么我们基于这个 <a href="https://stackoverflow.com/a/16562028/1573766">StackOverflow 回答</a> 中所建议的方法来过滤异常值。</p>
</blockquote>
<h4>语言流行度比较</h4>
<p>首先，让我们对所有的语言进行直接的比较，看看哪一种语言在前 100 个博客中有最多的浏览量。</p>
<p>这是能进行这个任务的函数：</p>
<div class="highlight"><pre><span></span><code>def get_languages_popularity(data):
    query_sorted_data = sorted(data, key=itemgetter(&#39;query&#39;))
    result = {&#39;languages&#39;: [], &#39;views&#39;: []}
    popularity = []
    for k, group in groupby(query_sorted_data, key=itemgetter(&#39;query&#39;)):
        group = list(group)
        daily_page_views = map(lambda r: int(r[&#39;daily_page_views&#39;]), group)
        total_page_views = sum(daily_page_views)
        popularity.append((group[0][&#39;query&#39;], total_page_views))
    sorted_popularity = sorted(popularity, key=itemgetter(1), reverse=True)
    languages, views = zip(*sorted_popularity)
    result[&#39;languages&#39;] = languages
    result[&#39;views&#39;] = views
    return result
</code></pre></div>

<p>在这里，我们首先按语言（词典中的关键字“query”）来分组我们的数据，然后使用 python 的 <code>groupby</code> 函数，这是一个从 SQL 中借来的奇妙函数，从我们的数据列表中生成一组条目，每个条目都表示一些编程语言。然后，在第 <code>14</code> 行我们计算每一种语言的总页面浏览量，然后添加 <code>('Language', rank)</code> 形式的元组到 <code>popularity</code> 列表中。在循环之后，我们根据总浏览量对流行度数据进行排序，并将这些元组展开到两个单独的列表中，然后在 <code>result</code> 变量中返回它们。</p>
<blockquote>
<p>最初的数据集有很大的偏差。我检查了到底发生了什么，并意识到如果我在 <a href="http://blogsearchengine.org/">blogsearchengine.org</a> 上查询“C”，我就会得到很多无关的链接，其中包含了 “C” 的字母。因此，我必须将 C 排除在分析之外。这种情况几乎不会在 “R” 和其他类似 C 的名称中出现：“C++”、“C”。</p>
</blockquote>
<p>因此，如果我们将 C 从考虑中移除并查看其他语言，我们可以看到如下图:</p>
<p><img alt="" src="/data/attachment/album/201711/09/181118vdtga5053aa9z9ta.png"></p>
<p>评估结论：Java 每天有超过 400 万的浏览量，PHP 和 Go 有超过 200 万，R 和 JavaScript 也突破了百万大关。</p>
<h4>每日网页浏览量与谷歌排名</h4>
<p>现在让我们来看看每日访问量和谷歌的博客排名之间的联系。从逻辑上来说，不那么受欢迎的博客应该排名靠后，但这并没那么简单，因为其他因素也会影响排名，例如，如果在人气较低的博客上的文章更新一些，那么它很可能会首先出现。</p>
<p>数据准备工作以下列方式进行：</p>
<div class="highlight"><pre><span></span><code>def get_languages_popularity(data):
    query_sorted_data = sorted(data, key=itemgetter(&#39;query&#39;))
    result = {&#39;languages&#39;: [], &#39;views&#39;: []}
    popularity = []
    for k, group in groupby(query_sorted_data, key=itemgetter(&#39;query&#39;)):
        group = list(group)
        daily_page_views = map(lambda r: int(r[&#39;daily_page_views&#39;]), group)
        total_page_views = sum(daily_page_views)
        popularity.append((group[0][&#39;query&#39;], total_page_views))
    sorted_popularity = sorted(popularity, key=itemgetter(1), reverse=True)
    languages, views = zip(*sorted_popularity)
    result[&#39;languages&#39;] = languages
    result[&#39;views&#39;] = views
    return result
</code></pre></div>

<p>该函数接受爬取到的数据和需要考虑的语言列表。我们对这些数据以语言的流行程度进行排序。后来，在类似的语言分组循环中，我们构建了 <code>(rank, views_number)</code> 元组（从 1 开始的排名）被转换为 2 个单独的列表。然后将这一对列表写入到生成的字典中。</p>
<p>前 8 位 GitHub 语言（除了 C）是如下这些：</p>
<p><img alt="" src="/data/attachment/album/201711/09/181142lofi8ol3awlzugsz.png"></p>
<p><img alt="" src="/data/attachment/album/201711/09/181205envskfnfnxzc3v3t.png"></p>
<p>评估结论：我们看到，所有图的 <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">PCC （皮尔逊相关系数）</a>都远离 1/-1，这表示每日浏览量与排名之间缺乏相关性。值得注意的是，在大多数图表（8 个中的 7 个）中，相关性是负的，这意味着排名的降低会导致浏览量的减少。</p>
<h3>结论</h3>
<p>因此，根据我们的分析，Java 是目前最流行的编程语言，其次是 PHP、Go、R 和 JavaScript。在日常浏览量和谷歌排名上，排名前 8 的语言都没有很强的相关性，所以即使你刚刚开始写博客，你也可以在搜索结果中获得很高的评价。不过，成为热门博客究竟需要什么，可以留待下次讨论。</p>
<blockquote>
<p>这些结果是相当有偏差的，如果没有更多的分析，就不能过分的考虑这些结果。首先，在较长的一段时间内收集更多的流量信息，然后分析每日浏览量和排名的平均值（中值）值是一个好主意。也许我以后还会再回来讨论这个。</p>
</blockquote>
<h3>引用</h3>
<ol>
<li>抓取：<ol>
<li><a href="https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/">blog.scrapinghub.com: Handling Javascript In Scrapy With Splash</a></li>
<li><a href="http://www.blogsearchengine.org/">BlogSearchEngine.org</a></li>
<li><a href="https://www.twingly.com/">twingly.com: Twingly Real-Time Blog Search</a></li>
<li><a href="http://www.searchblogspot.com/">searchblogspot.com: finding blogs on blogspot platform</a></li>
</ol>
</li>
<li>流量评估：<ol>
<li><a href="https://www.labnol.org/internet/find-website-traffic-hits/8008/">labnol.org: Find Out How Much Traffic a Website Gets</a></li>
<li><a href="https://www.quora.com/What-are-the-best-free-tools-that-estimate-visitor-traffic-for-a-given-page-on-a-particular-website-that-you-do-not-own-or-operate-3rd-party-sites">quora.com: What are the best free tools that estimate visitor traffic…</a></li>
<li><a href="http://www.statshow.com/">StatShow.com: The Stats Maker</a></li>
</ol>
</li>
</ol>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>