<!DOCTYPE html>
<html lang="zh">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>用 Spark SQL 进行结构化数据处理</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="description" content="Author: Phani Kiran Spark SQL 是 Spark 生态系统中处理结构化格式数据的模块。它在内部使用 Spark Core API 进行处理，但对用户的使用进行了抽象。这 …" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">linux_cn</a></h1>
                <nav><ul>
                    <li><a href="/category/chuan-shan-jia-zhuan-fang">穿山甲专访</a></li>
                    <li><a href="/category/dai-ma-ying-xiong">代码英雄</a></li>
                    <li><a href="/category/fen-xiang">分享</a></li>
                    <li><a href="/category/guan-dian">观点</a></li>
                    <li><a href="/category/huo-dong">活动</a></li>
                    <li><a href="/category/ji-ke-man-hua">极客漫画</a></li>
                    <li><a href="/category/ji-zhu">技术</a></li>
                    <li><a href="/category/kai-yuan-zhi-hui">开源智慧</a></li>
                    <li><a href="/category/linux-fa-xing-ban">Linux 发行版</a></li>
                    <li><a href="/category/mei-ri-an-quan-zi-xun">每日安全资讯</a></li>
                    <li><a href="/category/misc">Misc</a></li>
                    <li><a href="/category/qu-kuai-lian">区块链</a></li>
                    <li><a href="/category/rong-qi-yu-yun">容器与云</a></li>
                    <li class="active"><a href="/category/ruan-jian-kai-fa">软件开发</a></li>
                    <li><a href="/category/shu-mei-pai">树莓派</a></li>
                    <li><a href="/category/xi-tong-yun-wei">系统运维</a></li>
                    <li><a href="/category/xin-wen">新闻</a></li>
                    <li><a href="/category/ying-he-guan-cha">硬核观察</a></li>
                    <li><a href="/category/zhi-ye-sheng-ya">职业生涯</a></li>
                    <li><a href="/category/zhong-jiang-ming-dan">中奖名单</a></li>
                    <li><a href="/category/zhuo-mian-ying-yong">桌面应用</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/2022/05/yong-spark-sql-jin-xing-jie-gou-hua-shu-ju-chu-li.html" rel="bookmark"
           title="Permalink to 用 Spark SQL 进行结构化数据处理">用 Spark SQL 进行结构化数据处理</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2022-05-24T09:30:38+02:00">
                Published: Tue 24 May 2022
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/linux-fans-from-china.html">linux fans from china</a>
        </address>
<p>In <a href="/category/ruan-jian-kai-fa">软件开发</a>.</p>

</footer><!-- /.post-info -->      <p>Author: Phani Kiran</p>
<blockquote>
<p>Spark SQL 是 Spark 生态系统中处理结构化格式数据的模块。它在内部使用 Spark Core API 进行处理，但对用户的使用进行了抽象。这篇文章深入浅出地告诉你 Spark SQL 3.x 的新内容。</p>
</blockquote>
<p><img alt="" src="/data/attachment/album/202205/24/093036xaf6kaz1auaf4a7s.jpg"></p>
<p>有了 Spark SQL，用户可以编写 SQL 风格的查询。这对于精通结构化查询语言或 SQL 的广大用户群体来说，基本上是很有帮助的。用户也将能够在结构化数据上编写交互式和临时性的查询。Spark SQL 弥补了<ruby> 弹性分布式数据集 <rt>  resilient distributed data sets </rt></ruby>（RDD）和关系表之间的差距。RDD 是 Spark 的基本数据结构。它将数据作为分布式对象存储在适合并行处理的节点集群中。RDD 很适合底层处理，但在运行时很难调试，程序员不能自动推断<ruby> 模式 <rt>  schema </rt></ruby>。另外，RDD 没有内置的优化功能。Spark SQL 提供了<ruby> 数据帧 <rt>  DataFrame </rt></ruby>和数据集来解决这些问题。</p>
<p>Spark SQL 可以使用现有的 Hive 元存储、SerDes 和 UDF。它可以使用 JDBC/ODBC 连接到现有的 BI 工具。</p>
<h3>数据源</h3>
<p>大数据处理通常需要处理不同的文件类型和数据源（关系型和非关系型）的能力。Spark SQL 支持一个统一的数据帧接口来处理不同类型的源，如下所示。</p>
<ul>
<li>文件：<ul>
<li>CSV</li>
<li>Text</li>
<li>JSON</li>
<li>XML</li>
</ul>
</li>
<li>JDBC/ODBC：<ul>
<li>MySQL</li>
<li>Oracle</li>
<li>Postgres</li>
</ul>
</li>
<li>带模式的文件：<ul>
<li>AVRO</li>
<li>Parquet</li>
</ul>
</li>
<li>Hive 表：<ul>
<li>Spark SQL 也支持读写存储在 Apache Hive 中的数据。</li>
</ul>
</li>
</ul>
<p>通过数据帧，用户可以无缝地读取这些多样化的数据源，并对其进行转换/连接。</p>
<h3>Spark SQL 3.x 的新内容</h3>
<p>在以前的版本中（Spark 2.x），查询计划是基于启发式规则和成本估算的。从解析到逻辑和物理查询计划，最后到优化的过程是连续的。这些版本对转换和行动的运行时特性几乎没有可见性。因此，由于以下原因，查询计划是次优的：</p>
<ul>
<li>缺失和过时的统计数据</li>
<li>次优的启发式方法</li>
<li>错误的成本估计</li>
</ul>
<p>Spark 3.x 通过使用运行时数据来迭代改进查询计划和优化，增强了这个过程。前一阶段的运行时统计数据被用来优化后续阶段的查询计划。这里有一个反馈回路，有助于重新规划和重新优化执行计划。</p>
<p><img alt="Figure 1: Query planning" src="/data/attachment/album/202205/24/093039bgv1g1kw54xbk211.jpg"></p>
<h4>自适应查询执行（AQE）</h4>
<p>查询被改变为逻辑计划，最后变成物理计划。这里的概念是“重新优化”。它利用前一阶段的可用数据，为后续阶段重新优化。正因为如此，整个查询的执行要快得多。</p>
<p>AQE 可以通过设置 SQL 配置来启用，如下所示（Spark 3.0 中默认为 false）：</p>
<div class="highlight"><pre><span></span><code>spark.conf.set(“spark.sql.adaptive.enabled”,true)
</code></pre></div>

<h4>动态合并“洗牌”分区</h4>
<p>Spark 在“<ruby> 洗牌 <rt>  shuffle </rt></ruby>”操作后确定最佳的分区数量。在 AQE 中，Spark 使用默认的分区数，即 200 个。这可以通过配置来启用。</p>
<div class="highlight"><pre><span></span><code>spark.conf.set(“spark.sql.adaptive.coalescePartitions.enabled”,true)
</code></pre></div>

<h4>动态切换连接策略</h4>
<p>广播哈希是最好的连接操作。如果其中一个数据集很小，Spark 可以动态地切换到广播连接，而不是在网络上“洗牌”大量的数据。</p>
<h4>动态优化倾斜连接</h4>
<p>如果数据分布不均匀，数据会出现倾斜，会有一些大的分区。这些分区占用了大量的时间。Spark 3.x 通过将大分区分割成多个小分区来进行优化。这可以通过设置来启用：</p>
<div class="highlight"><pre><span></span><code>spark.conf.set(“spark.sql.adaptive.skewJoin.enabled”,true)
</code></pre></div>

<p><img alt="Figure 2: Performance improvement in Spark 3.x (Source: Databricks)" src="/data/attachment/album/202205/24/093039mz91rb31jiyt3qjc.jpg"></p>
<h3>其他改进措施</h3>
<p>此外，Spark SQL 3.x还支持以下内容。</p>
<h4>动态分区修剪</h4>
<p>3.x 将只读取基于其中一个表的值的相关分区。这消除了解析大表的需要。</p>
<h4>连接提示</h4>
<p>如果用户对数据有了解，这允许用户指定要使用的连接策略。这增强了查询的执行过程。</p>
<h4>兼容 ANSI SQL</h4>
<p>在兼容 Hive 的早期版本的 Spark 中，我们可以在查询中使用某些关键词，这样做是完全可行的。然而，这在 Spark SQL 3 中是不允许的，因为它有完整的 ANSI SQL 支持。例如，“将字符串转换为整数”会在运行时产生异常。它还支持保留关键字。</p>
<h4>较新的 Hadoop、Java 和 Scala 版本</h4>
<p>从 Spark 3.0 开始，支持 Java 11 和 Scala 2.12。 Java 11 具有更好的原生协调和垃圾校正，从而带来更好的性能。 Scala 2.12 利用了 Java 8 的新特性，优于 2.11。</p>
<p>Spark 3.x 提供了这些现成的有用功能，而无需开发人员操心。这将显着提高 Spark 的整体性能。</p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>